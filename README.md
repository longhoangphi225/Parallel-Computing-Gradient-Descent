# Parallel-Computing-Gradient-Descent
We commonly have to discover the least (or sometimes the greatest) value of a function in Machine Learning and Optimum Math in general (loss function). Finding the global minimum of loss functions in Machine Learning is, in general, extremely difficult, if not impossible. Instead, one frequently seeks out the local minimum and considers it to be the desired answer to the problem.

The local minimum points are the solutions of the zero derivative equation. If we can somehow find all (finite) minimum points, we just need to substitute each of those local minimum points into the function and then find the point. make the function minimum value. However, in most cases it is not possible to solve the zero derivative equation. The cause can come from the complexity of the form of the derivative, from having a large number of data points, or from having too many data points. The most common approach is to start from a point that we consider close to the solution of the problem, and then use an iterative operation to progress to the desired point, i.e., until the derivative is close to 0. Gradient Descent (G.D. for short) and its variations are among the most used.

However, with the advancement of the Internet, the amount of data we need to process has grown significantly, resulting in extremely long computation times. The author will use parallel computing approaches to increase the computational speed of the GD algorithm (particularly in the challenge of creating a Linear Regression model) in the content of this essay.
